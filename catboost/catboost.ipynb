{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End CatBoost\n",
    "\n",
    "©️2025 MetaSnake\n",
    "\n",
    "## https://github.com/mattharrison/odsc_east_2025\n",
    "\n",
    "`@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "* **Native categorical handling with ordered target statistics** — no manual encoding needed, leakage‑free\n",
    "* **Oblivious (symmetric) trees** that speed up training and prediction while reducing overfitting\n",
    "* **Strong GPU support** for both training and inference out of the box\n",
    "* **Robust default hyper‑parameters** that usually work well with minimal tuning\n",
    "* **Built‑in text, numerical, and categorical feature fusion**, including automatic text embedding\n",
    "* **Monotonicity and other feature constraints** to inject prior knowledge easily\n",
    "* **Efficient handling of missing values** without extra preprocessing\n",
    "* **Integrated SHAP‑based interpretability tools** for global and local explanations\n",
    "* **Snapshot and automatic resume** to safeguard long GPU/CPU training runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Handling\n",
    "\n",
    "CatBoost natively handles categorical features without the need for manual encoding. It uses a technique called \"ordered target statistics\" to compute statistics for categorical features.\n",
    "\n",
    "This method works by turning each categorical feature into integer hash. It then shuffles the data and computes the mean of the target variable for each category by iterating over the rows and only using the rows that have been seen so far. This way, it avoids data leakage. If it has seen a category before, it uses the mean of the target variable for that category (generally .5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "catboost.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "I'll be demoing with Kaggle 2018 survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "local = 'kaggle-survey-2018.zip'\n",
    "if not os.path.exists(local):\n",
    "    url = 'https://github.com/mattharrison/datasets/raw/master/data/kaggle-survey-2018.zip'\n",
    "    fin = urllib.request.urlopen(url)\n",
    "    data = fin.read()\n",
    "    with open(local, mode='wb') as fout:\n",
    "        fout.write(data)\n",
    "with zipfile.ZipFile(local) as z:\n",
    "    print(z.namelist())\n",
    "    kag = pd.read_csv(z.open('multipleChoiceResponses.csv'))\n",
    "    kag_questions = kag.iloc[0]\n",
    "    raw = kag.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.Q4.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.Q6.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "class TopNEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n=5, default='other'):\n",
    "        self.n = n\n",
    "        self.default = default\n",
    "        self.top_n_categories = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        counts = X.value_counts()\n",
    "        self.top_n_categories = set(counts.index[:self.n])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.where(X.isin(self.top_n_categories), self.default)\n",
    "\n",
    "class CustomOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop='first', prefix=None):\n",
    "        self.drop = drop\n",
    "        self.prefix = prefix\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.encoder = OneHotEncoder(drop=self.drop, sparse_output=False)\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        encoded = self.encoder.transform(X)#.values.reshape(-1, 1))\n",
    "        return encoded\n",
    "\n",
    "class AgeExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X\n",
    "                .assign(**{col: X[col].str.slice(0, 2).astype(int)\n",
    "                         for col in X.columns})\n",
    "        )\n",
    "\n",
    "class EducationEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mapping = {\n",
    "            'I prefer not to answer': None,\n",
    "            'No formal education past high school': 12,\n",
    "            'Some college/university study without earning a bachelor’s degree': 13,\n",
    "            'Bachelor’s degree': 16,\n",
    "            'Master’s degree': 18,\n",
    "            'Professional degree': 19,     \n",
    "            'Doctoral degree':20,\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        return X.map(self.mapping).to_frame()\n",
    "\n",
    "class ExperienceExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (pd.DataFrame(X)\n",
    "            .assign(\n",
    "                experience=lambda df: (\n",
    "                    df.iloc[:, 0]\n",
    "                    .str.replace('+', '', regex=False)\n",
    "                    .str.split('-', expand=True)\n",
    "                    .iloc[:, 0]\n",
    "                    .astype(float)\n",
    "                )\n",
    "            )\n",
    "            .loc[:, ['experience']]\n",
    "        )\n",
    "\n",
    "\n",
    "class CompensationExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (pd.DataFrame(X)\n",
    "            .assign(\n",
    "                compensation=lambda df: (\n",
    "                    df.iloc[:, 0]\n",
    "                    .str.replace('+', '', regex=False)\n",
    "                    .str.replace(',', '', regex=False)\n",
    "                    .str.replace('500000', '500', regex=False)\n",
    "                    .str.replace('I do not wish to disclose my approximate yearly compensation', '0', regex=False)\n",
    "                    .str.split('-', expand=True)\n",
    "                    .iloc[:, 0]\n",
    "                    .fillna(0)\n",
    "                    .astype(int)\n",
    "                    .mul(1000)\n",
    "                )\n",
    "            )\n",
    "            .loc[:, ['compensation']]\n",
    "        )\n",
    "\n",
    "class LanguageEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        return X.fillna(0).map({self.language: 1}).rename(self.language).to_frame()\n",
    "\n",
    "def create_pipeline():\n",
    "    gender_encoder = CustomOneHotEncoder(drop='first', prefix='gender')\n",
    "    country_encoder = CustomOneHotEncoder(drop='first', prefix='country')\n",
    "    major_encoder = Pipeline([\n",
    "        ('topn', TopNEncoder(n=3)),\n",
    "        ('replace', FunctionTransformer(lambda X: X.iloc[:, 0].map({\n",
    "            'Computer science (software engineering, etc.)': 'cs',\n",
    "            'Engineering (non-computer focused)': 'eng',\n",
    "            'Mathematics or statistics': 'stat'\n",
    "        }).to_frame())),\n",
    "        ('onehot', CustomOneHotEncoder(drop='first', prefix='major'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('gender', gender_encoder, ['Q1']),\n",
    "        ('age', AgeExtractor(), ['Q2']),\n",
    "        ('country', country_encoder, ['Q3']),\n",
    "        ('education', EducationEncoder(), ['Q4']),\n",
    "        ('major', major_encoder, ['Q5']),\n",
    "        ('years_exp', ExperienceExtractor(), ['Q8']),\n",
    "        ('compensation', CompensationExtractor(), ['Q9']),\n",
    "        ('python', LanguageEncoder('Python'), ['Q16_Part_1']),\n",
    "        ('r', LanguageEncoder('R'), ['Q16_Part_2']),\n",
    "        ('sql', LanguageEncoder('SQL'), ['Q16_Part_3'])\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# Usage\n",
    "pipeline = create_pipeline()\n",
    "kag_X_transformed = pipeline.fit_transform(\n",
    "    raw[raw.Q3.isin([\"United States of America\", \"China\", \"India\"])]\n",
    ")\n",
    "kag_y = (raw\n",
    "         .loc[kag_X_transformed.index]\n",
    "            .query('Q6 == \"Data Scientist\" or Q6 == \"Software Engineer\"')\n",
    "            .loc[:, 'Q6']\n",
    ")\n",
    "kag_X = kag_X_transformed.loc[kag_y.index]\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "kag_X_train, kag_X_test, kag_y_train, kag_y_test = train_test_split(\n",
    "    kag_X, kag_y, stratify=kag_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "kag_y.isna().pipe(lambda s: s[s > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA to Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_pipeline = Pipeline([('std', StandardScaler()), ('pca', PCA(n_components=3))])\n",
    "X_pca = pca_pipeline.fit_transform(kag_X)\n",
    "pca = pca_pipeline.named_steps['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components\n",
    "\n",
    "(pd.DataFrame(pca.components_, columns=kag_X.columns,\n",
    "              index=[f'pca{i}' for i in range(pca.n_components_ )])\n",
    ".loc[:, lambda df: (df.abs() > .1).any(axis='index')]\n",
    ".plot.bar()\n",
    ".legend(bbox_to_anchor=(1,1))              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(data:=X_pca.assign(**kag_X),\n",
    "                x='pca0', y='pca1', z='pca2',\n",
    "                #color='age__Q2',\n",
    "                #color='major__experience',\n",
    "                #color='education__Q4',\n",
    "                color='compensation__compensation',\n",
    "                  hover_data=data.columns,\n",
    "                color_continuous_scale='viridis')\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    "    title='3D PCA Scatter Plot'\n",
    ")      \n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color by job title\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(data:=X_pca.assign(**kag_X),\n",
    "                x='pca0', y='pca1', z='pca2',\n",
    "                color=kag_y, hover_data=data.columns,\n",
    "                color_continuous_scale='viridis')\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    "    title='3D PCA Scatter Plot'\n",
    ")      \n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stumps, Trees, and Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (raw.loc[\n",
    "    raw.Q3.isin([\"United States of America\", \"China\", \"India\"])\n",
    "    & raw.Q6.isin([\"Data Scientist\", \"Software Engineer\"]),\n",
    "    ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q8', 'Q9', 'Q16_Part_1', 'Q16_Part_2', 'Q16_Part_3']]\n",
    "    .rename(columns={'Q1': 'gender', 'Q2': 'age', 'Q3': 'country', 'Q4': 'education', \n",
    "                     'Q5': 'major', 'Q8': 'years_exp', 'Q9': 'compensation', \n",
    "                     'Q16_Part_1': 'python', 'Q16_Part_2': 'r', 'Q16_Part_3': 'sql'})\n",
    "    .fillna('NA')  # categories can't have missing values\n",
    "    # not strictly required to convert to category, but it will save memory (generally)\n",
    "    #.pipe(lambda df: df.assign(**df.select_dtypes('object').astype('category')))    \n",
    ")\n",
    "y = (raw\n",
    "        .loc[X.index]\n",
    "        .loc[:, 'Q6']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump = catboost.CatBoostClassifier(iterations=1, depth=1, \n",
    "                                    cat_features=list(X_train.columns))\n",
    "stump.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on an underfit model, we generally see similar (bad) performance on the training set\n",
    "stump.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fails without a CatBoost \"Pool\"\n",
    "stump.plot_tree(tree_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On codespaces run:\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install graphviz\n",
    "pool = catboost.Pool(X_train, y_train, cat_features=list(X_train.columns))\n",
    "res = stump.plot_tree(tree_idx=0, pool=pool)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the keys so that the order is consistent\n",
    "# .get_all_params() returns all of the parameters\n",
    "unsorted_dict = stump.get_all_params()\n",
    "sorted_dict = dict(sorted(unsorted_dict.items()))\n",
    "sorted_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit\n",
    "\n",
    "A stump is too simple. It has too much *bias*.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "- Add better features\n",
    "- Use a more complex model\n",
    "\n",
    "If we let the tree grow, it will do both of these things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit\n",
    "\n",
    "A model that is too complex has too much *variance*.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "- Simplify the model (or constrain/regularize it)\n",
    "- Use more data\n",
    "\n",
    "If a tree is too complex, we can prune it so that the leaf nodes are not too specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_settings = catboost_config = {\n",
    "    'auto_class_weights': 'None',\n",
    "    #'bayesian_matrix_reg': 0,  # Reduced from 0.10000000149011612\n",
    "    'best_model_min_trees': 1,\n",
    "    'boost_from_average': False,\n",
    "    'boosting_type': 'Plain',\n",
    "    #'bootstrap_type': 'No',  # Changed from 'MVS'\n",
    "    'border_count': 1024, #254,\n",
    "    #'class_names': ['Data Scientist', 'Software Engineer'],\n",
    "    'classes_count': 0,\n",
    "    'combinations_ctr': ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1',\n",
    "     'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1'],\n",
    "    'counter_calc_method': 'SkipTest',\n",
    "    'ctr_leaf_count_limit': 18446744073709551615,\n",
    "    'ctr_target_border_count': 1,\n",
    "    #'depth': None,  # Changed from 1 to None (unlimited depth)\n",
    "    'eval_fraction': 0,\n",
    "    'eval_metric': 'Logloss',\n",
    "    'feature_border_type': 'GreedyLogSum',\n",
    "    'fold_permutation_block': 0,\n",
    "    #'force_unit_auto_pair_weights': False,\n",
    "    'grow_policy': 'SymmetricTree',\n",
    "    'has_time': False,\n",
    "    #'iterations': 1000,  # Increased from 1\n",
    "    'l2_leaf_reg': 0,  # Reduced from 3\n",
    "    'leaf_estimation_backtracking': 'No',  # Changed from 'AnyImprovement'\n",
    "    'leaf_estimation_iterations': 10,  # Increased from 1\n",
    "    'leaf_estimation_method': 'Newton',\n",
    "    'learning_rate': 1,  # Increased from 0.5\n",
    "    'loss_function': 'Logloss',\n",
    "    'max_ctr_complexity': 8,  # Increased from 1\n",
    "    'max_leaves': None, #31,  # Increased from 2\n",
    "    'min_data_in_leaf': 1,\n",
    "    'model_shrink_mode': 'Constant',\n",
    "    'model_shrink_rate': 0,\n",
    "    'model_size_reg': 0,  # Reduced from 0.5\n",
    "    'nan_mode': 'Min',\n",
    "    'one_hot_max_size': 2,\n",
    "    'penalties_coefficient': 1,\n",
    "    #'permutation_count': 1,  # Reduced from 4\n",
    "    #'pool_metainfo_options': {'tags': {}},\n",
    "    'posterior_sampling': False,\n",
    "    'random_score_type': 'NormalWithModelSizeDecrease',\n",
    "    'random_seed': 0,\n",
    "    'random_strength': 0,  # Reduced from 1\n",
    "    'rsm': 1,\n",
    "    'sampling_frequency': 'PerTree',\n",
    "    'score_function': 'Cosine',\n",
    "    'simple_ctr': ['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1',\n",
    "     'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1'],\n",
    "    'sparse_features_conflict_fraction': 0,\n",
    "    'store_all_simple_ctr': False,\n",
    "    'subsample': 1,  # Increased from 0.800000011920929\n",
    "    'task_type': 'CPU',\n",
    "    'use_best_model': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails because CatBoost has a depth limit of 16\n",
    "hi_variance = catboost.CatBoostClassifier(iterations=1, depth=100, **var_settings,\n",
    "                                            cat_features=list(X_train.columns))\n",
    "hi_variance.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the tree fails if the depth > 13 (on my mac)\n",
    "hi_variance = catboost.CatBoostClassifier(iterations=1, depth=13, **var_settings,\n",
    "                        cat_features=list(X_train.columns))\n",
    "hi_variance.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_variance.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_variance.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Res={hi_variance._object._is_oblivious()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the tree is \"oblivious\". This means that at every level of the tree,\n",
    "# the feature that is split on is the same.\n",
    "# - These work better with categorical data\n",
    "# - They are also faster to train\n",
    "# - Easier to regularize\n",
    "hi_variance.plot_tree(tree_idx=0, pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks\n",
    "\n",
    "We want a model that is just right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "for i in range(1, 16):\n",
    "    goldilocks = catboost.CatBoostClassifier(iterations=1, depth=i, **var_settings,\n",
    "                                            cat_features=list(X_train.columns))\n",
    "    goldilocks.fit(X_train, y_train)\n",
    "    train_scores.append(goldilocks.score(X_train, y_train))\n",
    "    test_scores.append(goldilocks.score(X_test, y_test))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, 16), train_scores, label='train')\n",
    "plt.plot(range(1, 16), test_scores, label='test')\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold5 = catboost.CatBoostClassifier(iterations=1, depth=5, **var_settings,\n",
    "                                    cat_features=list(X_train.columns))\n",
    "gold5.fit(X_train, y_train)\n",
    "gold5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold5.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "CatBoost uses *boosting* to train a series of symmetric (oblivious) decision trees, where each tree tries to correct the mistakes made by the previous one. For classification tasks, this process refines the prediction into a probability.\n",
    "\n",
    "Imagine it like golfing: after each shot (tree), you adjust your next move to get closer to the hole (correct prediction). In contrast, a decision tree is like hitting one ball and stopping. A random forest would be like taking multiple tee shots and averaging them to find the best one.\n",
    "\n",
    "- *Automatic Handling of Categorical Features*: CatBoost natively supports categorical features, encoding them in a highly efficient way without the need for preprocessing.\n",
    "- *Missing Value Support*: CatBoost handles missing data for numeric data automatically. (It does not support missing categorical data.)\n",
    "- *Overfitting Detection*: To prevent overfitting, CatBoost can stop training early when it detects that the model is no longer improving.\n",
    "- *Embedding Features*: You can specify that a group of columns should be treated as an embedding feature, which can improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the out of the box model. Note that it performs much better than \n",
    "# goldilocks\n",
    "cat1 = catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False)\n",
    "cat1.fit(X_train, y_train)\n",
    "cat1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree 1\n",
    "cat1.plot_tree(tree_idx=0, pool=pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree 2\n",
    "cat1.plot_tree(tree_idx=1, pool=pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Hyperparameters\n",
    "\n",
    "### **Model Architecture and Tree Structure**\n",
    "\n",
    "- **`boosting_type='Plain'`**: Type of boosting algorithm. `'Plain'` means no boosting over time or residuals.\n",
    "  \n",
    "- **`grow_policy='SymmetricTree'`**: Strategy for growing trees. Symmetric trees split all branches at a given depth on the same feature, ensuring a balanced structure.\n",
    "\n",
    "- **`depth=6`**: Depth of the trees. Setting `None` means unlimited depth, allowing trees to grow without restrictions. \n",
    "\n",
    "- **`max_leaves=64`**: The maximum number of leaves in a tree. \n",
    "\n",
    "\n",
    "\n",
    "### **Feature Sampling and Processing**\n",
    "\n",
    "- **`rsm=1`**: Random subspace method. Controls the fraction of features considered for splits. Set to 1, meaning all features are used for splitting.\n",
    "\n",
    "- **`border_count=254`**: Number of splits for numeric features. A high value (1024) allows more precise splits. \n",
    "\n",
    "- **`max_ctr_complexity=4`**: Maximum complexity of combinatorial feature transformations for categorical features. \n",
    "\n",
    "- **`feature_border_type='GreedyLogSum'`**: Strategy for selecting borders (thresholds) when binning numeric features. `'GreedyLogSum'` uses a greedy logarithmic sum approach.\n",
    "\n",
    "- **`one_hot_max_size=2`**: Maximum number of unique categorical values for which one-hot encoding is used. For categories larger than this, other methods are applied.\n",
    "\n",
    "\n",
    "\n",
    "### **Categorical Feature Handling**\n",
    "\n",
    "- **`combinations_ctr=['Borders:CtrBorderCount=15:CtrBorderType=Uniform:TargetBorderCount=1:TargetBorderType=MinEntropy:Prior=0/1:Prior=0.5/1:Prior=1/1', 'Counter:CtrBorderCount=15:CtrBorderType=Uniform:Prior=0/1']`**: Control how categorical features are processed using combinatorial counters and borders.\n",
    "  \n",
    "- **`ctr_leaf_count_limit=18446744073709551615`**: Limits the maximum number of leaves used for categorical feature transformation. The large value effectively disables this limit.\n",
    "\n",
    "- **`ctr_target_border_count=1`**: Sets the number of target borders used in categorical target statistics. Helps to limit the complexity of categorical features.\n",
    "\n",
    "\n",
    "\n",
    "### **Regularization and Overfitting Prevention**\n",
    "\n",
    "- **`l2_leaf_reg=3`**: L2 regularization coefficient applied to leaf values. A lower value like 0 reduces regularization, allowing more variance. \n",
    "\n",
    "- **`random_strength=1`**: Strength of random noise added to splits' scores. Zero means no randomization, making the model more deterministic. \n",
    "\n",
    "- **`model_shrink_rate=0`**: Shrinkage rate for model weights during training. Zero means no shrinkage. \n",
    "\n",
    "- **`model_size_reg=0.5`**: Regularization coefficient for model size. A lower value like 0 disables this regularization. \n",
    "\n",
    "\n",
    "\n",
    "### **Tree Estimation and Leaf Calculations**\n",
    "\n",
    "- **`leaf_estimation_iterations=10`**: Number of gradient steps used to estimate the leaf values. \n",
    "\n",
    "- **`leaf_estimation_method='Newton'`**: Method for leaf value estimation. Uses Newton-Raphson method for faster convergence.\n",
    "\n",
    "- **`leaf_estimation_backtracking='AnyImprovement'`**: Specifies backtracking behavior during leaf value estimation. `'No'` disables any backtracking. \n",
    "\n",
    "\n",
    "\n",
    "### **Learning Rate and Convergence**\n",
    "\n",
    "- **`learning_rate=0.014`**: Step size used in gradient boosting. A higher learning rate speeds up training but risks overfitting. \n",
    "\n",
    "\n",
    "\n",
    "### **Loss Function and Metrics**\n",
    "\n",
    "- **`loss_function='Logloss'`**: Loss function used for binary classification, which minimizes the log loss.\n",
    "\n",
    "- **`eval_metric='Logloss'`**: Metric to evaluate during model training, matching the loss function used.\n",
    "\n",
    "\n",
    "\n",
    "### **Data Handling**\n",
    "\n",
    "- **`auto_class_weights='None'`**: No automatic class weight balancing. This assumes that class distribution in the data is balanced.\n",
    "\n",
    "- **`eval_fraction=0`**: Fraction of data used for evaluation. A value of `0` indicates no separate evaluation set.\n",
    "\n",
    "- **`subsample=.8`**: Fraction of data used for training each tree. A value of 1 means no subsampling. \n",
    "\n",
    "\n",
    "\n",
    "### **Cross-Validation and Permutations**\n",
    "\n",
    "- **`fold_permutation_block=0`**: Specifies block size for folding data during cross-validation. A value of 0 disables folding by blocks.\n",
    "\n",
    "- **`counter_calc_method='SkipTest'`**: Method used for calculating counter features. `'SkipTest'` means counters are calculated only on training data.\n",
    "\n",
    "- **`posterior_sampling=False`**: Disables posterior sampling for Bayesian-like updates of leaf values.\n",
    "\n",
    "\n",
    "\n",
    "### **Scoring and Bootstrapping**\n",
    "\n",
    "- **`score_function='Cosine'`**: The scoring function used to evaluate splits. `'Cosine'` computes the cosine similarity between vectors.\n",
    "\n",
    "- **`sampling_frequency='PerTree'`**: Frequency for resampling data. `PerTree` means resampling occurs once per tree, which adds randomness.\n",
    "\n",
    "\n",
    "\n",
    "### **Bootstrapping and Shrinkage**\n",
    "\n",
    "- **`boost_from_average=False`**: Whether to initialize leaf values from the mean of the target variable. Setting it to `False` can help with imbalanced datasets.\n",
    "\n",
    "- **`best_model_min_trees=1`**: Minimum number of trees required to determine the best model. *Set to 1*.\n",
    "\n",
    "\n",
    "\n",
    "### **Randomness and Reproducibility**\n",
    "\n",
    "- **`random_seed=0`**: Seed for random number generation to ensure reproducibility.\n",
    "\n",
    "- **`random_score_type='NormalWithModelSizeDecrease'`**: Type of randomness to introduce in scoring during splits. This type introduces noise proportional to model size.\n",
    "\n",
    "\n",
    "\n",
    "### **Additional Configurations**\n",
    "\n",
    "- **`penalties_coefficient=1`**: Penalty coefficient applied to regularization terms, set to 1 for no additional penalties.\n",
    "\n",
    "- **`task_type='CPU'`**: Specifies the type of processor to use for training (CPU-based training).\n",
    "\n",
    "- **`use_best_model=False`**: Disables the automatic selection of the best model during training.\n",
    "\n",
    "- **`nan_mode='Min'`**: Specifies how to handle missing values (NaN). `'Min'` treats missing values as the smallest possible values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Parameters to tune?\n",
    "\n",
    "| Parameter | Description | CB1 | CB2 | CB_tut | AWS | Forecastegy | Optuna |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| `learning_rate` | Step size used in gradient boosting | X | X | X | X | X | |\n",
    "| `random_strength` | Strength of random noise added to splits' scores | X | X |  | X | | |\n",
    "| `one_hot_max_size` | Maximum number of unique categorical values for which one-hot encoding is used | X |  |  |  | | |\n",
    "| `l2_leaf_reg` | L2 regularization coefficient applied to leaf values | X | X | X | X | | |\n",
    "| `bagging_temperature` | Controls the intensity of sampling | X | X |  |  |  | |\n",
    "| `iterations` | Number of trees to build | X | X |  |  | X | |\n",
    "| `use_best_model` | Use the best model found during training |  | X |  |  | | |\n",
    "| `eval_metric` | Metric to evaluate during model training |  | X |  |  | | X |\n",
    "| `od_type` | Type of overfitting detector to use |  | X |  |  | | |\n",
    "| `od_pval` | Threshold for the overfitting detector |  | X |  |  | | |\n",
    "| `od_wait` | Number of iterations to continue training after overfitting is detected |  | X |  |  | | |\n",
    "| `depth` | Depth of the trees |  | X | X | X | X | X |\n",
    "| `border_count` | Number of splits for numeric features |  | X |  |  | | |\n",
    "| `has_time` | Use time as a feature (data has time order) |  | X |  |  | | |\n",
    "| `grow_policy` | Strategy for growing trees |  | X |  |  | | |\n",
    "| `min_data_in_leaf` | Minimum number of training samples in a leaf |  | X |  |  | X | |\n",
    "| `max_leaves` | Maximum number of leaves in a tree |  | X |  |  | | |\n",
    "| `per_float_feature_quantization` | Number of bits to use for quantizing numerical features |  | X |  |  | | |\n",
    "| `max_ctr_complexity` | Maximum complexity of combinatorial feature transformations for categorical features |  |  | X |  | |  |  \n",
    "| `boosting_type` | Type of boosting algorithm |  |  | X |  | |  X |\n",
    "| `subsample` | Fraction of data used for training each tree |  |  |  |  | X | |\n",
    "| `col_sample_bylevel` | Fraction of features to consider for each level |  |  |  |  | X | X |\n",
    "| `bootstrap_type` | Sampling method for bagging |  |  |  |  | |  X |\n",
    "| `used_ram_limit` | Maximum amount of RAM to use for training |  |  |  |  | |  X |\n",
    "| `objective` | Objective function to optimize |  |  |  |  | | X |\n",
    "\n",
    "\n",
    "Taken from:\n",
    "- CB1 - CatBoost PDF \n",
    "- CB2 - https://catboost.ai/en/docs/concepts/parameter-tuning\n",
    "- CB_tut - https://github.com/catboost/tutorials/blob/master/hyperparameters_tuning/hyperparameters_tuning_using_optuna_and_hyperopt.ipynb\n",
    "- AWS - https://docs.aws.amazon.com/sagemaker/latest/dg/catboost-tuning.html\n",
    "- Forecastegy - https://forecastegy.com/posts/catboost-hyperparameter-tuning-guide-with-optuna/\n",
    "- Optuna - https://github.com/optuna/optuna-examples/blob/main/catboost/catboost_pruning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 45 seconds to run\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick import model_selection as ms\n",
    "from yellowbrick.utils import types, helpers\n",
    "from yellowbrick import base\n",
    "\n",
    "base.get_model_name = lambda model: 'CatBoost'\n",
    "#helpers.is_esitmator = lambda model: print(f'calling {model=}') or True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y, param_name='depth', param_range=range(1,6))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes about 90 seconds to run\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick import model_selection as ms\n",
    "from yellowbrick.utils import types, helpers\n",
    "from yellowbrick import base\n",
    "\n",
    "base.get_model_name = lambda model: 'CatBoost'\n",
    "#helpers.is_esitmator = lambda model: print(f'calling {model=}') or True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y, param_name='l2_leaf_reg', param_range=[0, 1, 3, 5, 10, 100])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes about 90 seconds to run\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick import model_selection as ms\n",
    "from yellowbrick.utils import types, helpers\n",
    "from yellowbrick import base\n",
    "\n",
    "base.get_model_name = lambda model: 'CatBoost'\n",
    "#helpers.is_esitmator = lambda model: print(f'calling {model=}') or True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y, param_name='l2_leaf_reg', param_range=[8, 10, 12, 20])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes about 90 seconds to run\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick import model_selection as ms\n",
    "from yellowbrick.utils import types, helpers\n",
    "from yellowbrick import base\n",
    "\n",
    "base.get_model_name = lambda model: 'CatBoost'\n",
    "#helpers.is_esitmator = lambda model: print(f'calling {model=}') or True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y, param_name='min_data_in_leaf', param_range=[1, 2, 5, 10, 20])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import model_selection\n",
    "# this takes a while to run (about 2 minutes)\n",
    "# can set scoring in GridSearchCV to \n",
    "# recall, precision, f1, accuracy\n",
    "params = {\n",
    "          'learning_rate': [.1, .3], # makes each boost more conservative (0 - no shrinkage) \n",
    "          'random_strength': [.5, 1, 2],\n",
    "          'one_hot_max_size': [1, 32, 64],\n",
    "          #'gamma': [0, 1],\n",
    "          'l2_leaf_reg': [0, 1, 2],\n",
    "          \n",
    "          'bagging_temperature': [0, 1],\n",
    "          #'early_stopping_rounds':[10],\n",
    "          'n_estimators': [200]}\n",
    "cb3 = catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False)\n",
    "cv = (model_selection.GridSearchCV(cb3, params, cv=3)#, n_jobs=-1)\n",
    "    .fit(X, y)\n",
    "#         eval_set=[(kag_X_test, kag_y_test)],\n",
    " #        early_stopping_rounds=5, verbose=10) \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bagging_temperature': 0,\n",
    " 'l2_leaf_reg': 2,\n",
    " 'learning_rate': 0.1,\n",
    " 'n_estimators': 200,\n",
    " 'one_hot_max_size': 1,\n",
    " 'random_strength': 0.5}\n",
    "cb4 = catboost.CatBoostClassifier(**params, cat_features=list(X_train.columns), verbose=False)\n",
    "cb4.fit(X_train, y_train)\n",
    "cb4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cb_def = catboost.CatBoostClassifier( cat_features=list(X_train.columns), verbose=True)\n",
    "cb_def.fit(X_train, y_train)\n",
    "cb_def.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note with tuned XGBoost model, I get 0.7253814147018031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we've tuned our model, let's look at how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, cb4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, cb4.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb4.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.precision_score(y_test, cb4.predict(X_test), pos_label='Software Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.recall_score(y_test, cb4.predict(X_test), pos_label='Software Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test, cb4.predict(X_test), pos_label='Software Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.ConfusionMatrixDisplay.from_estimator(cb4,\n",
    "                       X_test, y_test,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(cb4,\n",
    "                       X_test, y_test,ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(cb4,\n",
    "                       X_test, y_test,ax=ax)\n",
    "metrics.RocCurveDisplay.from_estimator(cb4,\n",
    "                       X_train, y_train,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(cb_def,\n",
    "                       X_test, y_test,ax=ax)\n",
    "metrics.RocCurveDisplay.from_estimator(cb_def,\n",
    "                       X_train, y_train,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(stump,\n",
    "                       X_test, y_test,ax=ax)\n",
    "metrics.RocCurveDisplay.from_estimator(stump,\n",
    "                       X_train, y_train,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(hi_variance,\n",
    "                       X_test, y_test,ax=ax)\n",
    "metrics.RocCurveDisplay.from_estimator(hi_variance,\n",
    "                       X_train, y_train,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.PrecisionRecallDisplay.from_estimator(cb4,\n",
    "                       X_test, y_test,ax=ax)\n",
    "metrics.PrecisionRecallDisplay.from_estimator(cb4,\n",
    "                       X_train, y_train,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "Tune for different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes about 45 seconds to run\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick import model_selection as ms\n",
    "from yellowbrick.utils import types, helpers\n",
    "from yellowbrick import base\n",
    "\n",
    "base.get_model_name = lambda model: 'CatBoost'\n",
    "#helpers.is_esitmator = lambda model: print(f'calling {model=}') or True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y, param_name='depth', param_range=range(1,6))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This complains because of y not being a number...\n",
    "# addressed in next cell\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y, param_name='depth', param_range=range(1,6),\n",
    "                    scoring='recall', pos_label='Software Engineer')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y=='Software Engineer', param_name='depth', param_range=range(1,6),\n",
    "                    scoring='recall')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(catboost.CatBoostClassifier(cat_features=list(X_train.columns), verbose=False),\n",
    "                    X, y=='Software Engineer', param_name='depth', param_range=range(1,6),\n",
    "                    scoring='precision')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bagging_temperature': 0,\n",
    " 'l2_leaf_reg': 2,\n",
    " 'learning_rate': 0.1,\n",
    " 'n_estimators': 200,\n",
    " 'one_hot_max_size': 1,\n",
    " 'random_strength': 0.5}\n",
    "cb4 = catboost.CatBoostClassifier(**params, cat_features=list(X_train.columns), verbose=False)\n",
    "cb4.fit(X_train, y_train)\n",
    "cb4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb4.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cb4.feature_importances_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(cb4.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_ranks(df_):\n",
    "    return (df_\n",
    "            .assign(**{f'{col}_rank':df_[col].rank(ascending=False) for col in \n",
    "                     df_.columns})\n",
    "           )\n",
    "    \n",
    "pool = catboost.Pool(X_train, y_train, cat_features=list(X_train.columns))\n",
    "(pd.DataFrame({typ:cb4.get_feature_importance(type=typ, data=pool)\n",
    "             for typ in ['PredictionValuesChange',\n",
    "                        'LossFunctionChange',\n",
    "                        'FeatureImportance', \n",
    "                         #'ShapValues',\n",
    "        #'ShapInteractionValues',\n",
    "        #'Interaction',\n",
    "        #'PredictionDiff',\n",
    "        #'SageValues'\n",
    "                        ]},\n",
    "            index=X_train.columns)\n",
    " .pipe(add_ranks)\n",
    " .sort_values(by='FeatureImportance_rank')\n",
    " .loc[:, 'PredictionValuesChange_rank':]\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP (SHapley Additive exPlantations)\n",
    "Should be *globally* consistent and accurate\n",
    "\n",
    " Shapley value (SHAP).\n",
    " \n",
    " From game theory, indicates how to distribute attribution of label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(cb4)\n",
    "vals = shap_ex(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(vals, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'major'], ax=ax, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'major'], ax=ax, x_jitter=.5, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic interaction\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'major'], ax=ax, color=vals, x_jitter=.5, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify interaction\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'major'], ax=ax, color=vals[:, 'education'], x_jitter=.5, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try R\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'r'], ax=ax, color=vals, x_jitter=.5, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try education\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'education'], ax=ax, color=vals, x_jitter=.5, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try education\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "shap.plots.scatter(vals[:,'years_exp'], ax=ax, color=vals, x_jitter=.5, alpha=.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# blue - DS\n",
    "# red - SE\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots.waterfall(vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb4.predict(X_test.iloc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "CatBoost is very powerful.\n",
    "\n",
    "Explore your data and your results.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "* Pandas skills come in useful for manipulating data\n",
    "* Make sure you discuss business value with stake holders\n",
    "\n",
    "\n",
    "Questions?\n",
    "\n",
    "\n",
    "Connect on LinkedIn or Twitter `@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odsc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
